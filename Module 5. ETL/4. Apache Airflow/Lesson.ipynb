{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Airflow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Talk about workflow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Introduce Airflow as a workflow manager"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It uses DAGs, put an image of a DAG where each node corresponds to a task"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Installing airflow would be as simple as running `pip install apache-airflow`, however, that migh cause dependency errors. Thus, in order to prevent those errors, run the following commands in your __`terminal`__ (gitbash if you are on Windows). At the time of writing, the version of Airflow is 2.1.2, if you are going to use a different version, change it in the following code:\n",
    "\n",
    "```\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "AIRFLOW_VERSION=2.1.2\n",
    "PYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you are storing the values of Airflow and your Python version in two variables that are going to be used in the following command:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"```\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you will get the corresponding version of Airflow from their github repo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"```\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you can start using airflow. It is a good idea to initialize your database now. This database will contain metadata and it will host the DAGs you create:\n",
    "\n",
    "`airflow db init`\n",
    "\n",
    "And pass your credentials:\n",
    "\n",
    "```\n",
    "airflow users create \\\n",
    "    --username <your_username> \\\n",
    "    --firstname <your_firstname> \\\n",
    "    --lastname <your_lastname> \\\n",
    "    --role Admin \\\n",
    "    --email <your_email>\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check everything went right. In the terminal, run:\n",
    "\n",
    "`airflow webserver --port 8080`\n",
    "\n",
    "This will start a new server at your localhost at port 8080. It is important to notice that, even if you start the server, your scheduled DAGs won't be monitored. To do so, we need to kick off the scheduler, so open a new terminal and run:\n",
    "\n",
    "`airflow scheduler`\n",
    "\n",
    "If you receive a Warning message, don't worry, it won't affect your airflow current performance. Now, we are ready to start using the UI"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, if you go to your browser and visit: `localhost:8080`, you should be able to see something like this:\n",
    "\n",
    "![](images/Airflow.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The image above depicts the Airflow UI. Here, you can see the DAGs that have been created, and so far, you will only see some examples and tutorials created by the Airflow team. Let's explore it a little bit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Airflow UI"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inside the UI, you can explore the metadata of the DAGs, such as the name (or ID), the owner, the status of previous runs of the whole DAG or of specific tasks inside the DAG, its frequency (in the Schedule column), and when it was ran the last time.\n",
    "\n",
    "You can see more details by clicking on the DAG. Let's observe the `example_bash_operator` DAG\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow2.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the DAG you can see its structure, the average time it took for running each task, the Gantt chart of the DAG to check if there are overlapping tasks, the details of the DAG, and the code that generated the DAG. We haven't ran this DAG yet, so there is no info about previous runs. We can, however, take a look at the code. Before, that, let's observe the `Graph View` tab, which will contain the same as the `Tree View` tab, but rearranged:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow4.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observe that we have several Nodes, each one representing a task. You can also observe their dependencies, for example, `run_after_loop` won't start until all `runme_x` haven't finished. \n",
    "\n",
    "Let's run a single task to see how it works.\n",
    "\n",
    "1. In the Airflow UI enable the `example_bash_operator` DAG. \n",
    "2. Click the DAG to see its status. You should see that there are two runs, this is because (as we will see later) these examples are set to be ran 2 days ago. If you observe the schedule, it is meant to run once every day, so two runs make sense!\n",
    "3. Inside those runs, there are different status, in this case, we can see 'success' and 'skipped'. Don't worry, they are meant to be skipped.\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow4.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Let's see its flow by triggering an event. First click Auto-refresh to see updates in real time, and then, click the Play button:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_clip.gif\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pretty cool, isn't it? We can also see the durantion of each task, and when each run took place. But I will let you explore more on that in the UI. For now, let's take a look at the code. If you click on the `Code` tab, you will see this:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"Example DAG demonstrating the usage of the BashOperator.\"\"\"\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "args = {\n",
    "    'owner': 'airflow',\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='example_bash_operator',\n",
    "    default_args=args,\n",
    "    schedule_interval='0 0 * * *',\n",
    "    start_date=days_ago(2),\n",
    "    dagrun_timeout=timedelta(minutes=60),\n",
    "    tags=['example', 'example2'],\n",
    "    params={\"example_key\": \"example_value\"},\n",
    ") as dag:\n",
    "\n",
    "    run_this_last = DummyOperator(\n",
    "        task_id='run_this_last',\n",
    "    )\n",
    "\n",
    "    # [START howto_operator_bash]\n",
    "    run_this = BashOperator(\n",
    "        task_id='run_after_loop',\n",
    "        bash_command='echo 1',\n",
    "    )\n",
    "    # [END howto_operator_bash]\n",
    "\n",
    "    run_this >> run_this_last\n",
    "\n",
    "    for i in range(3):\n",
    "        task = BashOperator(\n",
    "            task_id='runme_' + str(i),\n",
    "            bash_command='echo \"{{ task_instance_key_str }}\" && sleep 1',\n",
    "        )\n",
    "        task >> run_this\n",
    "\n",
    "    # [START howto_operator_bash_template]\n",
    "    also_run_this = BashOperator(\n",
    "        task_id='also_run_this',\n",
    "        bash_command='echo \"run_id={{ run_id }} | dag_run={{ dag_run }}\"',\n",
    "    )\n",
    "    # [END howto_operator_bash_template]\n",
    "    also_run_this >> run_this_last\n",
    "\n",
    "# [START howto_operator_bash_skip]\n",
    "this_will_skip = BashOperator(\n",
    "    task_id='this_will_skip',\n",
    "    bash_command='echo \"hello world\"; exit 99;',\n",
    "    dag=dag,\n",
    ")\n",
    "# [END howto_operator_bash_skip]\n",
    "this_will_skip >> run_this_last\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dag.cli()\n",
    "```"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the tasks were supposed to print out something to the console, we can check that on the Log tab of each task. For example, look at the `also_run_this` task, it is a BashOperator object that will print out `run_id={{ run_id }} | dag_run={{ dag_run }}`. Go to the `Graph View` tab and click on the `also_run_this` task, and in the next window click `Log`. Observe the output:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_log.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This was a simple walkthrough to show the Airflow UI. You saw that:\n",
    "\n",
    "- The workflow is represented by a DAG\n",
    "- Each node in the DAG corresponds to a task\n",
    "- Each DAG has a schedule that sets the frequency of runs\n",
    "- Tasks can be triggered by previous tasks\n",
    "- Each task corresponds to an operator object\n",
    "- We saw BashOperator, which execute a bash script\n",
    "- We saw DummyOperator, which according to the documentation _'Operator that does literally nothing. It can be used to group tasks in a DAG.'_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will see how to create more operators, for example, a PythonOperator, in the next section. First, let's get some practice defining a DAG with the operators we have seen so far."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating Your First DAG"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all, make sure you followed all steps so far. If that's the case, you should have a folder in your home directory named airflow. _Check it by running running the following cell. If no error is thrown, you are good to go_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from os.path import expanduser\n",
    "import os\n",
    "\n",
    "home = expanduser(\"~\")\n",
    "airflow_dir = os.path.join(home, 'airflow')\n",
    "assert os.path.isdir(airflow_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inside that directory, you have to add a new folder named `dags`. Airflow will look into that directory to check if the DAGs you create through Python. Now, the example DAGs you are using are in your PATH directory, but new DAGs you create should be placed in `~/airflow/dags/`. _You can actually change the path where Airflow will look for new DAGs in the airflow.cfg file_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from airflow.models import DAG"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from os.path import expanduser\n",
    "from pathlib import Path\n",
    "home = expanduser(\"~\")\n",
    "airflow_dir = os.path.join(home, 'airflow')\n",
    "Path(f\"{airflow_dir}/dags\").mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Python files you create have to be stored in that folder. The file should contain the DAG with the desired arguments. The arguments can be passed in the context manager and in a dictionary.\n",
    "\n",
    "In the context manager, simply define the tasks, don't implement any logical flow. As saw above, tasks are defined by operators"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from airflow.operators.bash_operator import BashOperator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "default_args = {\n",
    "    'owner': 'Ivan',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['ivan@theaicore.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'start_date': datetime(2020, 1, 1),\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'end_date': datetime(2022, 1, 1),\n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'wait_for_downstream': False,\n",
    "    # 'dag': dag,\n",
    "    # 'trigger_rule': 'all_success'\n",
    "}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with DAG(dag_id='test_dag',\n",
    "         default_args=default_args,\n",
    "         schedule_interval='*/1 * * * *',\n",
    "         catchup=False,\n",
    "         tags=['test']\n",
    "         ) as dag:\n",
    "    # Define the tasks. Here we are going to define only one bash operator\n",
    "    test_task = BashOperator(\n",
    "        task_id='write_date_file',\n",
    "        bash_command='cd ~/Desktop && date >> ai_core.txt',\n",
    "        dag=dag)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example can be found in the `examples` folder, under the name `dag_test.py`. Copy the example to your `dags` folder in your airflow directory.\n",
    "\n",
    "Once the file is in the airflow directory, you can run it by running the following command (if you haven't started the scheduler yet, run `airflow scheduler -D`):\n",
    "\n",
    "`airflow dags unpause test_dag`\n",
    "\n",
    "If you want these DAGs to appear in the UI, you have to add them by running the following command:\n",
    "\n",
    "`airflow db init`\n",
    "\n",
    "So you can manage them in the UI."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tasks Dependencies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You just created a task, but in a workflow, you will probablu need to add more than one, so let's add a few more. If you just specify the tasks, the tasks will be executed in sequence, with no specific order(you can change how they are executed by changing the executor in the airflow.cfg file). However, you can specify hoe they are ordered by setting the dependencies between them.\n",
    "\n",
    "Setting dependencies is quite simple. You can specify the tasks and then 'connect' them using the bit-shift operator. For example, if you want to run the task `runme_1` after the task `runme_0`, you can do it like this:\n",
    "\n",
    "`task_0 >> task_1` or `task_0.set_downstream(task_1)` or `task_1 << task_0` or `task_1.set_upstream(task_0)`.\n",
    "\n",
    "You can see that there are many ways to set the dependencies, so just pick the one that works for you.\n",
    "\n",
    "Let's say the you want to run both task `task_1` and `task_2` after task `task_0` has finished. You can do it like this:\n",
    "\n",
    "`task_0 >> [task_1, task_2]`\n",
    "\n",
    "Another thing you my want to do is running `task_2` only when both `task_0` and `task_1` have finished. You can do it like this:\n",
    "```\n",
    "task_0 >> task_2\n",
    "task_1 >> task_2\n",
    "```\n",
    "\n",
    "Finally, you can also set sequencial dependencies between tasks. For example, if you want to run `task_2` after `task_1`, and `task_1` after `task_0` have finished, you can do it like this:\n",
    "\n",
    "`task_0 >> task_1 >> task_2`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The example below shows a DAG with four tasks:\n",
    "\n",
    "1. date_task: A BashOperator that appends the current date into a file\n",
    "2. add_task: A BashOperator that stages the file created by date_task\n",
    "3. commit_task: A BashOperator that commits the file staged by add_task\n",
    "4. push_task: A BashOperator that pushes the committed file to a remote repository"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Ivan',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['ivan@theaicore.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'start_date': datetime(2020, 1, 1), # If you set a datetime previous to the curernt date, it will try to backfill\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'end_date': datetime(2022, 1, 1),\n",
    "}\n",
    "with DAG(dag_id='test_dag_dependencies',\n",
    "         default_args=default_args,\n",
    "         schedule_interval='*/1 * * * *',\n",
    "         catchup=False,\n",
    "         tags=['test']\n",
    "         ) as dag:\n",
    "    # Define the tasks. Here we are going to define only one bash operator\n",
    "    date_task = BashOperator(\n",
    "        task_id='write_date',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && date >> date.txt',\n",
    "        dag=dag)\n",
    "    add_task = BashOperator(\n",
    "        task_id='add_files',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git add .',\n",
    "        dag=dag)\n",
    "    commit_task = BashOperator(\n",
    "        task_id='commit_files',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git commit -m \"Update date\"',\n",
    "        dag=dag)\n",
    "    push_task = BashOperator(\n",
    "        task_id='push_files',\n",
    "        bash_command='cd ~/Desktop/Weather_Airflow && git push',\n",
    "        dag=dag)\n",
    "    \n",
    "    date_task >> add_task >> commit_task\n",
    "    add_task >> push_task\n",
    "    commit_task >> push_task"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observe the last part of the DAG, you can see the dependencies between the tasks. Of course, you could simply set them all in tandem, but in this case, you will see how to set the dependencies in different ways.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_Dependencies.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After running it, you will see that your repo is being updated every minute (which might be confusing, but this is a demo, so we don't care)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AirFlow_GitHub.png\" width=\"500\"/>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Try it out\n",
    "\n",
    "1. Create a new remote repository in your GitHub account. \n",
    "2. You will eventually use if for storing weather data, so name your repository accordingly.\n",
    "3. Clone the repository to your local machine.\n",
    "4. Copy the DAG file `dag_test_dependencies.py` to the folder `dags` in your local machine.\n",
    "5. Change the file according to the name of your repository and the directory you cloned it to.\n",
    "6. Unpause the DAG by running `airflow dags unpause dag_test_dependencies` or by going to the `DAGS` tab in the UI and clicking on the `dag_test_dependencies` DAG."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you start creating DAGs, you might forget which one are active. Good thing is that airflow has many commands to check your works in the command line. If you type `airflow -h` you can see all comands."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%%bash\n",
    "airflow -h"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "usage: airflow [-h] GROUP_OR_COMMAND ...\n",
      "\n",
      "positional arguments:\n",
      "  GROUP_OR_COMMAND\n",
      "\n",
      "    Groups:\n",
      "      celery         Celery components\n",
      "      config         View configuration\n",
      "      connections    Manage connections\n",
      "      dags           Manage DAGs\n",
      "      db             Database operations\n",
      "      jobs           Manage jobs\n",
      "      kubernetes     Tools to help run the KubernetesExecutor\n",
      "      pools          Manage pools\n",
      "      providers      Display providers\n",
      "      roles          Manage roles\n",
      "      tasks          Manage tasks\n",
      "      users          Manage users\n",
      "      variables      Manage variables\n",
      "\n",
      "    Commands:\n",
      "      cheat-sheet    Display cheat sheet\n",
      "      info           Show information about current Airflow and environment\n",
      "      kerberos       Start a kerberos ticket renewer\n",
      "      plugins        Dump information about loaded plugins\n",
      "      rotate-fernet-key\n",
      "                     Rotate encrypted connection credentials and variables\n",
      "      scheduler      Start a scheduler instance\n",
      "      sync-perm      Update permissions for existing roles and optionally DAGs\n",
      "      version        Show the version\n",
      "      webserver      Start a Airflow webserver instance\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help         show this help message and exit\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus, you can look at the dags by running"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%%bash\n",
    "airflow dags list"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dag_id                                  | filepath                                                                                                    | owner   | paused\n",
      "========================================+=============================================================================================================+=========+=======\n",
      "aicore_test                             | ai_test.py                                                                                                  | airflow | True  \n",
      "aicore_test2                            | ai_test2.py                                                                                                 | airflow | True  \n",
      "example_bash_operator                   | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py                   | airflow | False \n",
      "example_branch_datetime_operator_2      | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_branch_datetime_operator.py        | airflow | True  \n",
      "example_branch_dop_operator_v3          | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_branch_python_dop_operator_3.py    | airflow | True  \n",
      "example_branch_labels                   | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_branch_labels.py                   | airflow | True  \n",
      "example_branch_operator                 | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_branch_operator.py                 | airflow | True  \n",
      "example_complex                         | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_complex.py                         | airflow | True  \n",
      "example_dag_decorator                   | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_dag_decorator.py                   | airflow | True  \n",
      "example_external_task_marker_child      | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_external_task_marker_dag.py        | airflow | True  \n",
      "example_external_task_marker_parent     | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_external_task_marker_dag.py        | airflow | True  \n",
      "example_kubernetes_executor             | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_kubernetes_executor.py             | airflow | True  \n",
      "example_nested_branch_dag               | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_nested_branch_dag.py               | airflow | True  \n",
      "example_passing_params_via_test_command | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_passing_params_via_test_command.py | airflow | True  \n",
      "example_python_operator                 | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_python_operator.py                 | airflow | True  \n",
      "example_short_circuit_operator          | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_short_circuit_operator.py          | airflow | True  \n",
      "example_skip_dag                        | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_skip_dag.py                        | airflow | True  \n",
      "example_subdag_operator                 | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_subdag_operator.py                 | airflow | True  \n",
      "example_subdag_operator.section-1       | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_subdag_operator.py                 | airflow | True  \n",
      "example_subdag_operator.section-2       | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_subdag_operator.py                 | airflow | True  \n",
      "example_task_group                      | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_task_group.py                      | airflow | True  \n",
      "example_task_group_decorator            | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_task_group_decorator.py            | airflow | True  \n",
      "example_trigger_controller_dag          | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_trigger_controller_dag.py          | airflow | True  \n",
      "example_trigger_target_dag              | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_trigger_target_dag.py              | airflow | True  \n",
      "example_weekday_branch_operator         | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_branch_day_of_week_operator.py     | airflow | True  \n",
      "example_xcom                            | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_xcom.py                            | airflow | True  \n",
      "example_xcom_args                       | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_xcomargs.py                        | airflow | True  \n",
      "example_xcom_args_with_operators        | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_xcomargs.py                        | airflow | True  \n",
      "latest_only                             | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_latest_only.py                     | airflow | True  \n",
      "latest_only_with_trigger                | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/example_latest_only_with_trigger.py        | airflow | True  \n",
      "test_dag                                | dag_test.py                                                                                                 | Ivan    | True  \n",
      "test_dag_dependencies                   | dag_test_dependencies.py                                                                                    | Ivan    | True  \n",
      "test_utils                              | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/test_utils.py                              | airflow | True  \n",
      "tutorial                                | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/tutorial.py                                | airflow | False \n",
      "tutorial_etl_dag                        | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/tutorial_etl_dag.py                        | airflow | True  \n",
      "tutorial_taskflow_api_etl               | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/tutorial_taskflow_api_etl.py               | airflow | True  \n",
      "tutorial_taskflow_api_etl_virtualenv    | /opt/miniconda3/lib/python3.9/site-packages/airflow/example_dags/tutorial_taskflow_api_etl_virtualenv.py    | airflow | True  \n",
      "                                                                                                                                                                        \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Python Operators"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have seen that you can run bash commands in each task. Airflow is not limited to these commands, you can also create PythonOperators to do anything you, as long as it is contained in a Python function. Let's say that you want to create a PythonOperator that will extract information about events that took place 'On this day' in the past.\n",
    "\n",
    "The first thing you have to do is creating a function that uses requests and bs4 to download that information from Wikipedia. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def get_ul(url: str):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    return soup.find('ul')\n",
    "\n",
    "def get_today_events(url: str):\n",
    "    ul_soup = get_ul(url)\n",
    "    for li in ul_soup.find_all('li'):\n",
    "        if li.find('h3'):\n",
    "            print(li.find('h3').text)\n",
    "            print(li.find('p').text)\n",
    "\n",
    "get_ul('https://en.wikipedia.org/wiki/Wikipedia:On_this_day/Today')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<ul><li><a href=\"/wiki/1576\" title=\"1576\">1576</a> – The <a href=\"/wiki/Cornerstone\" title=\"Cornerstone\">cornerstone</a> of Danish astronomer <a href=\"/wiki/Tycho_Brahe\" title=\"Tycho Brahe\">Tycho Brahe</a>'s observatory <b><a href=\"/wiki/Uraniborg\" title=\"Uraniborg\">Uraniborg</a></b> was laid on the island of <a href=\"/wiki/Ven_(Sweden)\" title=\"Ven (Sweden)\">Hven</a>.</li>\n",
       "<li><a href=\"/wiki/1918\" title=\"1918\">1918</a> – The <b><a href=\"/wiki/Battle_of_Amiens_(1918)\" title=\"Battle of Amiens (1918)\">Battle of Amiens</a></b> began in <a href=\"/wiki/Amiens\" title=\"Amiens\">Amiens</a>, France, marking the start of the <a href=\"/wiki/Allies_of_World_War_I\" title=\"Allies of World War I\">Allied Powers</a>' <a href=\"/wiki/Hundred_Days_Offensive\" title=\"Hundred Days Offensive\">Hundred Days Offensive</a> through the German front lines that ultimately led to the end of <a href=\"/wiki/World_War_I\" title=\"World War I\">World War I</a>.</li>\n",
       "<li><a href=\"/wiki/1969\" title=\"1969\">1969</a> – At a <a href=\"/wiki/Zebra_crossing\" title=\"Zebra crossing\">zebra crossing</a> in London <i>(pictured)</i>, photographer <a href=\"/wiki/Iain_Macmillan\" title=\"Iain Macmillan\">Iain Macmillan</a> took the photo that was used for the cover of <a href=\"/wiki/The_Beatles\" title=\"The Beatles\">the Beatles</a>' album <i><b><a href=\"/wiki/Abbey_Road\" title=\"Abbey Road\">Abbey Road</a></b></i>.</li>\n",
       "<li><a href=\"/wiki/2008\" title=\"2008\">2008</a> – A <a href=\"/wiki/EuroCity\" title=\"EuroCity\">EuroCity</a> train en route to Prague <b><a href=\"/wiki/2008_Stud%C3%A9nka_train_wreck\" title=\"2008 Studénka train wreck\">struck a part of a motorway bridge</a></b> that had fallen onto the track near <a href=\"/wiki/Stud%C3%A9nka\" title=\"Studénka\">Studénka</a> station and derailed, killing 8 people and injuring 64 others.</li></ul>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
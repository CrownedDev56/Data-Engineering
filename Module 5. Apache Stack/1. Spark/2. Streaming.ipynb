{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "marked-newton",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "\n",
    "> Streaming is an extension to Spark's core which allows to perform __real life data processing__\n",
    "\n",
    "> __Not all functionalities are available on Python currently, in order to work with full powered framework one should use `scala` or `java`__\n",
    "\n",
    "![](./images/streaming-arch.png)\n",
    "\n",
    "Flow looks as below:\n",
    "1. We hook up to data stream source (e.g. `Apache Kafka`)\n",
    "2. Incoming data is divided into batches \n",
    "3. Batches are processed by `spark` engine\n",
    "4. Resulting data batches are returned (e.g. after `MapReduce` transforms)\n",
    "\n",
    "![](./images/streaming-flow.png)\n",
    "\n",
    "## DStreams (Discretized Streams)\n",
    "\n",
    "> High level abstraction over stream of data which allows us to easily work it\n",
    "\n",
    "These can be created either by:\n",
    "- Reading directly our data source\n",
    "- Modifying  existing `DStreams` (and creating new ones at the same time)\n",
    "\n",
    "> __`DStreams` internally are represented as a sequence of `RDD`s__\n",
    "\n",
    "Let's start by creating `Session` (__you should always start like that!__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b110f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import pyspark\n",
    "\n",
    "# We should always start with session in order to obtain\n",
    "# context and session if needed\n",
    "session = pyspark.sql.SparkSession.builder.config(\n",
    "    conf=pyspark.SparkConf()\n",
    "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
    "    .setAppName(\"TestApp\")\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e1ab1",
   "metadata": {},
   "source": [
    "## StreamingContext\n",
    "\n",
    "After that we can create `pyspark.streaming.StremingContext` ([docs](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.html)) object which:\n",
    "- takes `sparkContext` as input\n",
    "- it can be the one we're using for other things (SQL or core PySpark functionality)\n",
    "- __it does not work with `pyspark.SparkContext` object directly though!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71208e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# This context can be used with PySpark streaming\n",
    "# You might have to specify batchDuration (e.g. on which time window operation will be run)\n",
    "# By default data is collected every 0.5 seconds\n",
    "ssc = StreamingContext(session.sparkContext, batchDuration=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19cf3e",
   "metadata": {},
   "source": [
    "Important things to notice:\n",
    "- __We set up the whole computation pipeline first__\n",
    "- __NOTHING__ is started until we use `ssc.start()` and finish with `ssc.end()`\n",
    "\n",
    "Methods of interest which allow us to work with streams:\n",
    "- [`ssc.awaitTermination([timeout])`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.awaitTermination.html#pyspark.streaming.StreamingContext.awaitTermination)\n",
    "    processes data indefinitely or up to a moment `timeout` is hit\n",
    "- [`ssc.checkpoint(directory)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.checkpoint.html#pyspark.streaming.StreamingContext.checkpoint) -\n",
    "    periodically checkpoint data for increased fault tolerance\n",
    "- [`scc.getActiveOrCreate(path, function)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.getActiveOrCreate.html#pyspark.streaming.StreamingContext.getActiveOrCreate) -\n",
    "    If there is an active stream (`start`ed and not `stop`ped) return it, otherwise recreate if from checkpoint\n",
    "\n",
    "Methods we will run each time:\n",
    "- [`scc.start()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.start.html#pyspark.streaming.StreamingContext.start) - starts execution of streams\n",
    "- [`scc.stop()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.stop.html#pyspark.streaming.StreamingContext.stop) - stops executions of streams\n",
    "\n",
    "Creating streams from context:\n",
    "- [`ssc.socketTextStream(hostname, port [,storageLevel])`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.socketTextStream.html#pyspark.streaming.StreamingContext.socketTextStream) - create input stream by listening on specified `hostname` and `port`\n",
    "- [`ssc.textFileStreams(directory)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.textFileStream.html#pyspark.streaming.StreamingContext.textFileStream) - watch for new files created on Hadoop compatible system (e.g. HDFS) in specified directory and read them as text files\n",
    "- [`ssc.binaryRecordsStream(directory, recordLength)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.binaryRecordsStream.html#pyspark.streaming.StreamingContext.binaryRecordsStream) - as above, but reads the files as binary\n",
    "\n",
    "Given all of that, let's create a `DStream` via `socketTextStream` which will listen on `localhost` for data incoming to the machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will send lines of data to this socketTextStream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ba389",
   "metadata": {},
   "source": [
    "Let's also apply same transformations as previously which:\n",
    "- Will split input text into `words` and flatten the result\n",
    "- Count unique words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb71dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = lines.flatMap(lambda text: text.split()).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825eba0b",
   "metadata": {},
   "source": [
    "And print the incoming words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56375d",
   "metadata": {},
   "source": [
    "__Notice nothing happened yet!__\n",
    "\n",
    "This is due to our `computation` not starting yet. In order to do that, we have to run `ssc.start()`.\n",
    "\n",
    "Before we do that though, let's run `netcat` to push some data to our socket.\n",
    "\n",
    "Please notice that:\n",
    "- `--listen` flag is specified __which means we have created server listening on specified port__\n",
    "- PySpark's `socket` __expects to find server which it can connect to under specified address!__\n",
    "\n",
    "Let's run this interactive command. It will allow us to send text data to `localhost:9999` (make sure you have `nc` or `netcat` available on your system!).\n",
    "\n",
    "> __Run it in a separate cell in order not to block the execution of the notebook!__\n",
    "\n",
    "After you've run it you can send textual data to the server setup by `netcat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5356f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nc --listen -p 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f27888",
   "metadata": {},
   "source": [
    "Now, you can run `start` which:\n",
    "- __will run indefinitely__, BUT\n",
    "- __will NOT stop Python's program execution__\n",
    "\n",
    "Due to above we will be able to run next cell (in our program it would be next line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225bbc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17243c6f",
   "metadata": {},
   "source": [
    "Now we can run the cell below in order to wait for `seconds` until `pyspark` terminates the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d945d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds = 180\n",
    "\n",
    "ssc.awaitTermination(seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde6219",
   "metadata": {},
   "source": [
    "Finally, we can stop `pyspark` client handling incoming data.\n",
    "\n",
    "Specifying `stopGraceFully=True` will allow it to finish only after consuming whole data posted to `9999` port:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9bff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed4155a",
   "metadata": {},
   "source": [
    "## Analyze\n",
    "\n",
    "Let's analyze our results. This infographic will help us clear the confusion:\n",
    "\n",
    "![](./images/streaming-dstream.png)\n",
    "\n",
    "![](./images/streaming-dstream-ops.png)\n",
    "\n",
    "As one can see:\n",
    "\n",
    "- __Operations are done on the batch of gathered data, NOT ON THE WHOLE DATASET__\n",
    "- If we want to do that we should `persist` the results to disk periodically\n",
    "- Streams will be automatically cleared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82dcc9c",
   "metadata": {},
   "source": [
    "## Procedure to follow when working with Streams\n",
    "\n",
    "Here are the steps one usually employs when working with streams:\n",
    "\n",
    "1. Define `SparkStreamContext` from Session's context\n",
    "2. Define the input sources by creating input `DStreams`.\n",
    "3. Define the streaming computations by applying transformation and output operations to `DStreams`.\n",
    "4. Start receiving data and processing it using `streamingContext.start()`.\n",
    "5. Wait for the processing to be stopped (manually or due to any error) using `streamingContext.awaitTermination()`.\n",
    "6. The processing can be manually stopped using `streamingContext.stop()`.\n",
    "\n",
    "In addition we should remember (especially when working with multiple streams):\n",
    "\n",
    "1. __Once a context has been started, no new streaming computations can be set up or added to it__.\n",
    "2. Once a context has been stopped, it cannot be restarted.\n",
    "3. Only one `StreamingContext` can be active in a JVM at the same time.\n",
    "4. `stop()` on `StreamingContext` also stops the `SparkContext`. \n",
    "    To stop only the `StreamingContext`, set the optional parameter of `stop()` `stopSparkContext` to false.\n",
    "5. A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped \n",
    "    (without stopping the SparkContext) before the next StreamingContext is created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647f164",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "\n",
    "## Assessment\n",
    "\n",
    "- Check out [linking with Kafka](https://spark.apache.org/docs/latest/streaming-programming-guide.html#linking) in order to setup real data streaming procedure\n",
    "- Check out [additional receivers](https://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams-and-receivers) and see what one has to do in order to set it up\n",
    "\n",
    "## Non-assessment\n",
    "\n",
    "- Check how Spark monitors directories for new files [here](https://spark.apache.org/docs/latest/streaming-programming-guide.html#how-directories-are-monitored)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-AiCore] *",
   "language": "python",
   "name": "conda-env-.conda-AiCore-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
